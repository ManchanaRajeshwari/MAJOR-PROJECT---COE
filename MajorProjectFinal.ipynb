{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "XmlI0mPbwcKf"
      },
      "outputs": [],
      "source": [
        "!pip install streamlit google-generativeai pyngrok --quiet  #tHere are total three libraries\n",
        "#1.streamlit is for interacting with the web app directly from the python scripts\n",
        "#2.google generative is used to activate the gemini models\n",
        "#3.pyngork is used to create a secure public url to ouur localhost app\n",
        "#quiet --is used to avoid the unneccesary outputs and it keps clean"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyD8CpgGFraY-pZFKNN4CLjjGkHqHOhIX-c\"  # Replace with your Gemini API key\n"
      ],
      "metadata": {
        "id": "dA61eL2PwoxP"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile emotion_detector.py\n",
        "import google.generativeai as genai    #To interact with the Gemini models\n",
        "import os    #It is to get the access of the environment variables like API Keys\n",
        "\n",
        "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
        "model = genai.GenerativeModel(\"gemini-2.5-flash-lite\")   #This model is a lightweight and fast model and suitable for emotion classification tasks\n",
        "\n",
        "def detect_emotion(text):\n",
        "    prompt = f\"Detect the dominant emotion from this text: '{text}'. Respond with one word like 'fear', 'joy', 'sadness', 'anger', or 'surprise'.\"\n",
        "    response = model.generate_content(prompt)\n",
        "    return response.text.strip().lower()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKP6xpmjwrJG",
        "outputId": "39126aec-d785-4dd8-dcde-314bf0fc28cf"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting emotion_detector.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile story_generator.py\n",
        "import google.generativeai as genai\n",
        "import os  #It is used to fecth the environment variables\n",
        "\n",
        "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
        "model = genai.GenerativeModel(\"gemini-2.5-flash-lite\")   #Used for fast and efficient generation of story telling tasks\n",
        "\n",
        "def continue_story(scene, emotion):#The initial user prompt and the emotion\n",
        "    prompt = (\n",
        "        f\"Continue this story in the tone of {emotion}. Make it emotionally rich and realistic.\\n\\n\"\n",
        "        f\"Scene: {scene}\\n\\n\"\n",
        "        f\"Continuation:\"\n",
        "    )\n",
        "    response = model.generate_content(prompt)\n",
        "    return response.text.strip()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20oXxx4kwuKG",
        "outputId": "30c66b1e-1a4a-40ca-9f98-5b76f000f054"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting story_generator.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile image_generator.py\n",
        "from diffusers import StableDiffusionXLPipeline  #Used for loading and Generating the images\n",
        "import torch     #Used to detect and for fast generation\n",
        "\n",
        "# Load the model\n",
        "pipe = StableDiffusionXLPipeline.from_pretrained(\n",
        "    \"stabilityai/sdxl-turbo\",\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "    variant=\"fp16\" if torch.cuda.is_available() else None,\n",
        "    use_safetensors=True\n",
        ").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def generate_emotion_image(prompt, emotion):\n",
        "    styled_prompt = (\n",
        "        f\"Highly detailed, cinematic, ultra-realistic of a real human showing strong {emotion.lower()} emotion. \"\n",
        "        f\"{prompt}, expressive facial features matching the emotion, dramatic but natural lighting, realistic background, \"\n",
        "        f\"35mm photography, shallow depth of field, photorealism, 8k resolution\"\n",
        "    )\n",
        "    result = pipe(prompt=styled_prompt, guidance_scale=1.5, num_inference_steps=4)     # guidance_scale=1.5 is used to control how much the image should stick to the prompt\n",
        "    image = result.images[0]                                                          #num_inference_steps=4 is used to fast generation\n",
        "    filename = f\"{emotion.lower()}_image.png\"\n",
        "    image.save(filename)\n",
        "    return filename\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0z1IsNGwxLY",
        "outputId": "e75d5086-299c-45a8-ba77-8acc22441ff5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting image_generator.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile image_prompt_generator.py\n",
        "import google.generativeai as genai\n",
        "import os\n",
        "\n",
        "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
        "model = genai.GenerativeModel(\"gemini-2.5-flash-lite\")\n",
        "\n",
        "def generate_image_prompt(scene, emotion):\n",
        "    prompt = (\n",
        "        f\"Generate a detailed visual prompt for an image showing the emotion '{emotion}' in the following scene:\\n\\n\"\n",
        "        f\"{scene}\\n\\n\"\n",
        "        f\"Make sure the facial expressions and environment reflect the emotion clearly.\"\n",
        "    )\n",
        "    response = model.generate_content(prompt)\n",
        "    return response.text.strip()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNCmb9DE_RKo",
        "outputId": "54d2ae62-e421-4cd5-e418-595af3f62cdd"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting image_prompt_generator.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st    # is the web-interface\n",
        "from PIL import Image   # is used to display images\n",
        "from emotion_detector import detect_emotion\n",
        "from story_generator import continue_story\n",
        "from image_prompt_generator import generate_image_prompt\n",
        "from image_generator import generate_emotion_image  # Make sure this file exists\n",
        "\n",
        "st.set_page_config(page_title=\"Emotion-Aware Story Generator\", layout=\"centered\")\n",
        "st.title(\"ðŸŽ­ Emotion-Aware AI Story Assistant\")\n",
        "\n",
        "scene_input = st.text_area(\"Enter the opening scene:\", height=150)\n",
        "\n",
        "if st.button(\"Generate\"):\n",
        "    if not scene_input.strip():\n",
        "        st.error(\"Please enter some text to analyze.\")\n",
        "    else:\n",
        "        with st.spinner(\"Detecting emotion...\"):\n",
        "            emotion = detect_emotion(scene_input)\n",
        "            st.success(f\"Detected Emotion: **{emotion.capitalize()}**\")\n",
        "\n",
        "        with st.spinner(\"Generating continuation...\"):\n",
        "            continuation = continue_story(scene_input, emotion)\n",
        "            st.subheader(\"ðŸ“˜ Continued Story\")\n",
        "            st.write(continuation)\n",
        "\n",
        "        with st.spinner(\"Generating visual prompt...\"):\n",
        "            img_prompt = generate_image_prompt(scene_input, emotion)\n",
        "            st.text(\"Prompt used for image generation:\")\n",
        "            st.write(img_prompt)\n",
        "\n",
        "        with st.spinner(\"Generating image...\"):\n",
        "            image_path = generate_emotion_image(scene_input,emotion)\n",
        "            st.image(image_path, caption=f\"Generated image for '{emotion}'\", use_container_width=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWWscqHLzTA8",
        "outputId": "6dd65d2f-49b2-46c8-8248-cddf5a817957"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok config add-authtoken 30u73qXNft0Q62PVpksvxNtJpDL_3tcrS81bV65erRQhQXQkG"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0b8i7qa_6Ukj",
        "outputId": "f24d9a27-3e30-4ef4-c729-e33230450ee4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import os\n",
        "\n",
        "# Set your Ngrok authtoken\n",
        "ngrok.set_auth_token(\"30u73qXNft0Q62PVpksvxNtJpDL_3tcrS81bV65erRQhQXQkG\")\n",
        "\n",
        "# Kill any existing tunnels (just in case)\n",
        "ngrok.kill()\n",
        "\n",
        "# Expose the Streamlit port (7860)\n",
        "public_url = ngrok.connect(7860) # Pass the port directly\n",
        "print(f\"ðŸš€ Public link: {public_url}\")\n",
        "\n",
        "# Run the Streamlit app\n",
        "!streamlit run app.py --server.port 7860 --server.enableCORS false"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XTIU6cFzNmB",
        "outputId": "ad1bca1b-8d41-40d7-89fd-acc254b0231d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸš€ Public link: NgrokTunnel: \"https://1ec892d519cc.ngrok-free.app\" -> \"http://localhost:7860\"\n",
            "2025-08-06 08:30:07.316 \n",
            "Warning: the config option 'server.enableCORS=false' is not compatible with\n",
            "'server.enableXsrfProtection=true'.\n",
            "As a result, 'server.enableCORS' is being overridden to 'true'.\n",
            "\n",
            "More information:\n",
            "In order to protect against CSRF attacks, we send a cookie with each request.\n",
            "To do so, we must specify allowable origins, which places a restriction on\n",
            "cross-origin resource sharing.\n",
            "\n",
            "If cross origin resource sharing is required, please disable server.enableXsrfProtection.\n",
            "            \n",
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:7860\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:7860\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.123.110.214:7860\u001b[0m\n",
            "\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2025-08-06T08:30:11+0000 lvl=warn msg=\"failed to check for update\" obj=updater err=\"Post \\\"https://update.equinox.io/check\\\": context deadline exceeded\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-08-06 08:30:31.944493: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1754469032.019449   15554 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1754469032.043026   15554 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "Loading pipeline components...: 100% 7/7 [00:03<00:00,  2.14it/s]\n",
            "  0% 0/4 [00:00<?, ?it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OZzTNFpfSBHx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}